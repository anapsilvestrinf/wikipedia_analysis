{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Wikipedia Pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volunteer content contributors and editors maintain Wikipedia by continuously improving content. Because of this, Wikipedia has a huge library. In this project, we will implement a simplified version of the grep command-line utlity to search for data in 54 megabytes worth of articles scraped from Wikipedia. \n",
    "\n",
    "Our main goals are:\n",
    "\n",
    "* Search for all occurrences of a string in all of the files.\n",
    "* Provide a case-insensitive option to the search.\n",
    "* Refine the result by providing the specific locations of the files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing Wikipedia Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Articles were saved using the last component of their URLs and all data files are in the wiki forder. To start, we'll list all of the files in this folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in the wiki folder:  999\n"
     ]
    }
   ],
   "source": [
    "# List files in the wiki folder and count number of files\n",
    "import os\n",
    "\n",
    "folder = 'wiki'\n",
    "files = os.listdir(folder)\n",
    "number_files = len(files)\n",
    "print('Number of files in the wiki folder: ', number_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<!DOCTYPE html>\\n',\n",
       " '<html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\\n',\n",
       " '<head>\\n',\n",
       " '<meta charset=\"UTF-8\"/>\\n',\n",
       " '<title>Bay of Concepción - Wikipedia</title>\\n',\n",
       " '<script>document.documentElement.className = document.documentElement.className.replace( /(^|\\\\s)client-nojs(\\\\s|$)/, \"$1client-js$2\" );</script>\\n',\n",
       " '<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":false,\"wgNamespaceNumber\":0,\"wgPageName\":\"Bay_of_Concepción\",\"wgTitle\":\"Bay of Concepción\",\"wgCurRevisionId\":647460156,\"wgRevisionId\":647460156,\"wgArticleId\":16044270,\"wgIsArticle\":true,\"wgIsRedirect\":false,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"Coordinates on Wikidata\",\"All stub articles\",\"Landforms of Bío Bío Region\",\"Bays of Chile\",\"Bío Bío Region geography stubs\"],\"wgBreakFrames\":false,\"wgPageContentLanguage\":\"en\",\"wgPageContentModel\":\"wikitext\",\"wgSeparatorTransformTable\":[\"\",\"\"],\"wgDigitTransformTable\":[\"\",\"\"],\"wgDefaultDateFormat\":\"dmy\",\"wgMonthNames\":[\"\",\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"],\"wgMonthNamesShort\":[\"\",\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"],\"wgRelevantPageName\":\"Bay_of_Concepción\",\"wgRelevantArticleId\":16044270,\"wgRequestId\":\"WKq3wgpAAEIAAMFPZFwAAABQ\",\"wgIsProbablyEditable\":true,\"wgRestrictionEdit\":[],\"wgRestrictionMove\":[],\"wgFlaggedRevsParams\":{\"tags\":{}},\"wgStableRevisionId\":null,\"wgWikiEditorEnabledModules\":{\"toolbar\":true,\"dialogs\":true,\"preview\":false,\"publish\":false},\"wgBetaFeaturesFeatures\":[],\"wgMediaViewerOnClick\":true,\"wgMediaViewerEnabledByDefault\":true,\"wgVisualEditor\":{\"pageLanguageCode\":\"en\",\"pageLanguageDir\":\"ltr\",\"usePageImages\":true,\"usePageDescriptions\":true},\"wgPreferredVariant\":\"en\",\"wgMFDisplayWikibaseDescriptions\":{\"search\":true,\"nearby\":true,\"watchlist\":true,\"tagline\":true},\"wgRelatedArticles\":null,\"wgRelatedArticlesBetaFeatureEnabled\":false,\"wgRelatedArticlesUseCirrusSearch\":true,\"wgRelatedArticlesOnlyUseCirrusSearch\":false,\"wgULSCurrentAutonym\":\"English\",\"wgNoticeProject\":\"wikipedia\",\"wgCentralNoticeCookiesToDelete\":[],\"wgCentralNoticeCategoriesUsingLegacy\":[\"Fundraising\",\"fundraising\"],\"wgCategoryTreePageCategoryOptions\":\"{\\\\\"mode\\\\\":0,\\\\\"hideprefix\\\\\":20,\\\\\"showcount\\\\\":true,\\\\\"namespaces\\\\\":false}\",\"wgCoordinates\":{\"lat\":-36.683333333333,\"lon\":-73.033333333333},\"wgWikibaseItemId\":\"Q4874197\",\"wgCentralAuthMobileDomain\":false,\"wgVisualEditorToolbarScrollOffset\":0,\"wgEditSubmitButtonLabelPublish\":false});mw.loader.state({\"ext.globalCssJs.user.styles\":\"ready\",\"ext.globalCssJs.site.styles\":\"ready\",\"site.styles\":\"ready\",\"noscript\":\"ready\",\"user.styles\":\"ready\",\"user\":\"ready\",\"user.options\":\"loading\",\"user.tokens\":\"loading\",\"ext.cite.styles\":\"ready\",\"wikibase.client.init\":\"ready\",\"ext.visualEditor.desktopArticleTarget.noscript\":\"ready\",\"ext.uls.interlanguage\":\"ready\",\"ext.wikimediaBadges\":\"ready\",\"mediawiki.legacy.shared\":\"ready\",\"mediawiki.legacy.commonPrint\":\"ready\",\"mediawiki.sectionAnchor\":\"ready\",\"mediawiki.skinning.interface\":\"ready\",\"skins.vector.styles\":\"ready\",\"ext.globalCssJs.user\":\"ready\",\"ext.globalCssJs.site\":\"ready\"});mw.loader.implement(\"user.options@0j3lz3q\",function($,jQuery,require,module){mw.user.options.set({\"variant\":\"en\"});});mw.loader.implement(\"user.tokens@1dqfd7l\",function ( $, jQuery, require, module ) {\\n',\n",
       " 'mw.user.tokens.set({\"editToken\":\"+\\\\\\\\\",\"patrolToken\":\"+\\\\\\\\\",\"watchToken\":\"+\\\\\\\\\",\"csrfToken\":\"+\\\\\\\\\"});/*@nomin*/;\\n',\n",
       " '\\n',\n",
       " '});mw.loader.load([\"ext.cite.a11y\",\"mediawiki.action.view.postEdit\",\"site\",\"mediawiki.page.startup\",\"mediawiki.user\",\"mediawiki.hidpi\",\"mediawiki.page.ready\",\"mediawiki.legacy.wikibits\",\"mediawiki.searchSuggest\",\"ext.gadget.teahouse\",\"ext.gadget.ReferenceTooltips\",\"ext.gadget.watchlist-notice\",\"ext.gadget.DRN-wizard\",\"ext.gadget.charinsert\",\"ext.gadget.refToolbar\",\"ext.gadget.extra-toolbar-buttons\",\"ext.gadget.switcher\",\"ext.gadget.featured-articles-links\",\"ext.centralauth.centralautologin\",\"mmv.head\",\"mmv.bootstrap.autostart\",\"ext.visualEditor.desktopArticleTarget.init\",\"ext.visualEditor.targetLoader\",\"ext.eventLogging.subscriber\",\"ext.wikimediaEvents\",\"ext.navigationTiming\",\"ext.uls.eventlogger\",\"ext.uls.init\",\"ext.uls.interface\",\"ext.quicksurveys.init\",\"ext.centralNotice.geoIP\",\"ext.centralNotice.startUp\",\"skins.vector.js\"]);});</script>\\n']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the first file and print first ten lines\n",
    "with open(os.path.join(folder, files[0])) as first_file:\n",
    "    first_file = list(first_file)\n",
    "first_file[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the MapReduce Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will add the MapReduce Framework, given that it is the base of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import functools\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def make_chunks(data, num_chunks):\n",
    "    chunk_size = math.ceil(len(data) / num_chunks)\n",
    "    return [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]\n",
    "\n",
    "def map_reduce(data, num_processes, mapper, reducer):\n",
    "    chunks = make_chunks(data, num_processes)\n",
    "    pool = Pool(num_processes)\n",
    "    chunks_results = pool.map(mapper, chunks)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return functools.reduce(reducer, chunks_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total number of lines in all files in the wiki folder using MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499797"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def open_files(files):\n",
    "    len_files = 0\n",
    "    for name_file in files:\n",
    "        with open(os.path.join('wiki', name_file)) as open_file:\n",
    "            len_files += len(list(open_file))\n",
    "    return len_files\n",
    " \n",
    "def count_lines(len_files1, len_files2):\n",
    "    return len_files1 + len_files2\n",
    "\n",
    "map_reduce(files, 10, open_files, count_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grep Exact Match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first MapReduce grep algorithm will locate all lines in all files from the wiki folder that contains a given string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_string(files):\n",
    "    findings = {}\n",
    "    for file_name in files:\n",
    "        with open(os.path.join('wiki', file_name)) as opened_file:\n",
    "            file = list(opened_file)\n",
    "            for row_index in range(len(file)):\n",
    "                if str_given in file[row_index]:\n",
    "                    if file_name not in findings:\n",
    "                        findings[file_name] = [row_index]\n",
    "                    else:\n",
    "                        findings[file_name].append(row_index)\n",
    "    return findings\n",
    "\n",
    "def reduce_dict(dict1, dict2):\n",
    "    dict1.update(dict2)\n",
    "    return dict1\n",
    "        \n",
    "def grep_mapreduce(folder, num_process):\n",
    "    files_in_folder = os.listdir(folder)\n",
    "    return map_reduce(files_in_folder, num_process, find_string, reduce_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding occurences of the string 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_given = 'data'\n",
    "data_occurrences = grep_mapreduce('wiki', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grep Case Insensitive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make our previous solution case insensitive by converting both the string given and the file lowercase inside our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_string_insensitive(files):\n",
    "    findings = {}\n",
    "    for file_name in files:\n",
    "        with open(os.path.join('wiki', file_name)) as opened_file:\n",
    "            file = [row.lower() for row in opened_file.readlines()]\n",
    "            for row_index in range(len(file)):\n",
    "                if str_given.lower() in file[row_index]:\n",
    "                    if file_name not in findings:\n",
    "                        findings[file_name] = [row_index]\n",
    "                    else:\n",
    "                        findings[file_name].append(row_index)\n",
    "    return findings\n",
    "\n",
    "def grep_mapreduce_insensitive(folder, num_process):\n",
    "    files_in_folder = os.listdir(folder)\n",
    "    return map_reduce(files_in_folder, num_process, find_string_insensitive, reduce_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find all occurences of the string 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_given = 'data'\n",
    "data_occurrences_insensitive = grep_mapreduce_insensitive('wiki', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check if we find more matches with the last algorithm in each file since we have saved dictionaries for each function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table_Point_Formation.html: 1 new occurrences.\n",
      "Ingrid_GuimarC3A3es.html: 1 new occurrences.\n",
      "Jules_Verne_ATV.html: 2 new occurrences.\n",
      "Pictogram.html: 1 new occurrences.\n",
      "Claire_Danes.html: 2 new occurrences.\n",
      "PTPRS.html: 1 new occurrences.\n",
      "A_Beautiful_Valley.html: 1 new occurrences.\n",
      "Mudramothiram.html: 1 new occurrences.\n",
      "Gordon_Bau.html: 2 new occurrences.\n",
      "Embraer_Unidade_GaviC3A3o_Peixoto_Airport.html: 1 new occurrences.\n",
      "Code_page_1023.html: 3 new occurrences.\n",
      "Cryptographic_primitive.html: 1 new occurrences.\n",
      "Alex_Kurtzman.html: 1 new occurrences.\n",
      "Filip_Pyrochta.html: 1 new occurrences.\n",
      "Morgana_King.html: 1 new occurrences.\n",
      "Don_Parsons_(ice_hockey).html: 1 new occurrences.\n",
      "Bias.html: 1 new occurrences.\n",
      "Tomohiko_ItC58D_(director).html: 2 new occurrences.\n",
      "Imperial_Venus_(film).html: 1 new occurrences.\n",
      "Camp_Nelson_Confederate_Cemetery.html: 1 new occurrences.\n",
      "Benny_Lee.html: 1 new occurrences.\n",
      "Kul_Gul.html: 1 new occurrences.\n",
      "Medicago_murex.html: 1 new occurrences.\n",
      "Oldfield_Baby_Great_Lakes.html: 1 new occurrences.\n",
      "Wilson_Global_Explorer.html: 1 new occurrences.\n",
      "Craig_Chester.html: 1 new occurrences.\n",
      "Derek_Acorah.html: 1 new occurrences.\n",
      "Jack_Goes_Home.html: 1 new occurrences.\n",
      "Morning_Glory_(2010_film).html: 1 new occurrences.\n",
      "Tim_Spencer_(singer).html: 1 new occurrences.\n",
      "Lower_Blackburn_Grade_Bridge.html: 1 new occurrences.\n",
      "1953E2809354_FA_Cup_qualifying_rounds.html: 1 new occurrences.\n",
      "Sol_Eclipse.html: 1 new occurrences.\n",
      "Jonathan_A._Goldstein.html: 1 new occurrences.\n",
      "83_(number).html: 1 new occurrences.\n",
      "Devil_on_Horseback.html: 1 new occurrences.\n",
      "Harry_Hill_Bandholtz.html: 1 new occurrences.\n",
      "Shpolskii_matrix.html: 2 new occurrences.\n",
      "Dragnet_(franchise).html: 6 new occurrences.\n",
      "Qalat_Kat.html: 1 new occurrences.\n",
      "Maniitsoq_structure.html: 3 new occurrences.\n",
      "Ordinary_Virginia.html: 1 new occurrences.\n",
      "Dewoitine_D.21.html: 1 new occurrences.\n",
      "Furto_di_sera_bel_colpo_si_spera.html: 1 new occurrences.\n",
      "Rudy_The_Rudy_Giuliani_Story.html: 1 new occurrences.\n",
      "Exploratorium_(film).html: 1 new occurrences.\n",
      "Foulonia.html: 1 new occurrences.\n",
      "Amborella.html: 1 new occurrences.\n",
      "Rally_for_Democracy_and_Progress_(Benin).html: 1 new occurrences.\n",
      "Swathi_Chinukulu.html: 1 new occurrences.\n",
      "Precorrin6A_reductase.html: 2 new occurrences.\n",
      "The_Gentleman_Without_a_Residence_(1915_film).html: 1 new occurrences.\n",
      "Manhattan_Murder_Mystery.html: 1 new occurrences.\n",
      "Viva_Villa.html: 2 new occurrences.\n",
      "Companys_procC3A9s_a_Catalunya.html: 1 new occurrences.\n",
      "Avengers_Academy.html: 1 new occurrences.\n",
      "Antibiotic_use_in_livestock.html: 1 new occurrences.\n",
      "Syngenor.html: 1 new occurrences.\n",
      "Cobble_Hill_Brooklyn.html: 1 new occurrences.\n",
      "Typhoon_Hester_(1952).html: 1 new occurrences.\n",
      "WintersWimberley_House.html: 1 new occurrences.\n",
      "Kokan_Colony.html: 1 new occurrences.\n",
      "Wilhelm_Wagenfeld_House.html: 1 new occurrences.\n",
      "Taipa_HousesE28093Museum.html: 2 new occurrences.\n",
      "WLSR.html: 1 new occurrences.\n",
      "Lake_County_Examiner.html: 1 new occurrences.\n",
      "Copamyntis_infusella.html: 1 new occurrences.\n",
      "C11orf30.html: 1 new occurrences.\n",
      "Old_Mill_Creek_Illinois.html: 1 new occurrences.\n",
      "Bahmanabade_Olya.html: 1 new occurrences.\n",
      "Ek_Dil_Sau_Afsane.html: 1 new occurrences.\n",
      "Daniel_Cerone.html: 1 new occurrences.\n",
      "Shoreyjehye_Do.html: 1 new occurrences.\n",
      "Failing_Office_Building.html: 1 new occurrences.\n",
      "Pushkar.html: 1 new occurrences.\n",
      "List_of_Uzbek_films_of_2014.html: 1 new occurrences.\n",
      "KMTZ.html: 1 new occurrences.\n",
      "Golabkhvaran.html: 1 new occurrences.\n",
      "CurtissWright_Hangar_(Columbia_South_Carolina).html: 1 new occurrences.\n",
      "Blue_SWAT.html: 1 new occurrences.\n",
      "Danish_Maritime_Safety_Administration.html: 1 new occurrences.\n",
      "Don_Raye.html: 1 new occurrences.\n",
      "Lis_LC3B8wert.html: 1 new occurrences.\n",
      "Doumanaba.html: 1 new occurrences.\n",
      "Sahanpur.html: 1 new occurrences.\n",
      "Meleh_Kabude_Sofla.html: 1 new occurrences.\n",
      "Panchamrutham.html: 1 new occurrences.\n",
      "Bibiana_Beglau.html: 1 new occurrences.\n",
      "Kattukukke.html: 1 new occurrences.\n",
      "Acceptance_(Heroes).html: 1 new occurrences.\n",
      "Westchester_Los_Angeles.html: 1 new occurrences.\n",
      "Appa_(film).html: 1 new occurrences.\n",
      "HD_90156.html: 1 new occurrences.\n",
      "The_Audacity_to_Podcast.html: 2 new occurrences.\n",
      "Brownfield_(software_development).html: 1 new occurrences.\n",
      "Boardman_Township_Mahoning_County_Ohio.html: 1 new occurrences.\n",
      "King_Parker_House.html: 1 new occurrences.\n",
      "List_of_Spaghetti_Western_films.html: 2 new occurrences.\n",
      "The_Future_(film).html: 1 new occurrences.\n",
      "Weiser_River.html: 1 new occurrences.\n",
      "Jon_Mullich.html: 1 new occurrences.\n",
      "Saravan_Gilan.html: 1 new occurrences.\n",
      "Agaritine_gammaglutamyltransferase.html: 2 new occurrences.\n",
      "Nuno_Leal_Maia.html: 1 new occurrences.\n",
      "Battle_of_Wattignies.html: 1 new occurrences.\n",
      "Colchester_Village_Historic_District.html: 1 new occurrences.\n",
      "Hayateumi_Hidehito.html: 1 new occurrences.\n",
      "List_of_people_from_Bangor_Maine.html: 7 new occurrences.\n",
      "Mirisah.html: 1 new occurrences.\n",
      "Teiji_Ito.html: 1 new occurrences.\n",
      "L._Fry.html: 1 new occurrences.\n",
      "Tropical_sprue.html: 1 new occurrences.\n",
      "Roxbury_Presbyterian_Church.html: 1 new occurrences.\n",
      "Peter_Collingwood.html: 1 new occurrences.\n",
      "List_of_molecular_graphics_systems.html: 4 new occurrences.\n",
      "Functoid.html: 1 new occurrences.\n",
      "Vojin_C486etkoviC487.html: 1 new occurrences.\n",
      "Julien_Boisselier.html: 1 new occurrences.\n",
      "Jazz_in_Turkey.html: 1 new occurrences.\n",
      "Kim_Yonghwa.html: 2 new occurrences.\n",
      "Holly_Golightly_(comics).html: 1 new occurrences.\n",
      "SalemAuburn_Streets_Historic_District.html: 1 new occurrences.\n",
      "Kate_Harwood.html: 2 new occurrences.\n",
      "Gulliver_Mickey.html: 1 new occurrences.\n",
      "Urs_Burkart.html: 1 new occurrences.\n",
      "Smilax_laurifolia.html: 1 new occurrences.\n",
      "Taylor_Williamson.html: 1 new occurrences.\n",
      "Claudia_Neidig.html: 1 new occurrences.\n",
      "Dean_Kukan.html: 1 new occurrences.\n",
      "Demographics_of_American_Samoa.html: 1 new occurrences.\n",
      "C389cole_des_Mines_de_Douai.html: 1 new occurrences.\n",
      "Frost_Township_Michigan.html: 1 new occurrences.\n",
      "Shabbir_Kumar.html: 1 new occurrences.\n",
      "West_Park_Bridge.html: 1 new occurrences.\n"
     ]
    }
   ],
   "source": [
    "for file in data_occurrences:\n",
    "    if data_occurrences[file] != data_occurrences_insensitive[file]:\n",
    "        print(f'{file}: {len(data_occurrences_insensitive[file]) - len(data_occurrences[file])} new occurrences.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Match Positions on Lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, we were finding the line numbers where there is occurrence. The next implementation will extend the last algorithm to give information about the location of the matches in the line. \n",
    "\n",
    "We will alter the dictionary by returning a tuple in the list with the number of the line and the index of the first character of the match in the line in the list instead of just the number of the line. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subproblem with first_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will test how to find a word in a line before applying the code in our function. We'll do it with the sixth line (index 5) in the file stored in the first_file variable at the beginning of this project and the word 'client' that starts at index 13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<script>document.documentElement.className = document.documentElement.className.replace( /(^|\\\\s)client-nojs(\\\\s|$)/, \"$1client-js$2\" );</script>\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_file[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[96, 119]\n",
      "client\n",
      "client\n"
     ]
    }
   ],
   "source": [
    "def find_in_row(row, word):\n",
    "    index = row.find(word, 0)\n",
    "    occurrences = []\n",
    "    while index != -1:\n",
    "        occurrences.append(index)\n",
    "        index = row.find(word, index + 1)\n",
    "        \n",
    "    return occurrences\n",
    "\n",
    "print(find_in_row(first_file[5], 'client'))\n",
    "print(first_file[5][96:96 + len('client')])\n",
    "print(first_file[5][119:119 + len('client')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_string_insensit_pos(files):\n",
    "    findings = {}\n",
    "    for file_name in files:\n",
    "        with open(os.path.join('wiki', file_name)) as opened_file:\n",
    "            file = [row.lower() for row in opened_file.readlines()]\n",
    "            \n",
    "            for row_index in range(len(file)):\n",
    "                if str_given.lower() in file[row_index]:\n",
    "                    list_char_index = find_in_row(file[row_index], str_given)\n",
    "                    \n",
    "                    if file_name not in findings:\n",
    "                        findings[file_name] = [(row_index, i) for i in list_char_index]\n",
    "                    else:\n",
    "                        for i in list_char_index:\n",
    "                            findings[file_name].append((row_index, i))\n",
    "    return findings\n",
    "\n",
    "\n",
    "\n",
    "def grep_mapreduce_insensit_pos(folder, num_process):\n",
    "    files_in_folder = os.listdir(folder)\n",
    "    return map_reduce(files_in_folder, num_process, find_string_insensit_pos, reduce_dict)\n",
    "\n",
    "str_given = 'science'\n",
    "science_match = grep_mapreduce_insensit_pos('wiki', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can show our results in a CSV file for better visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# Number of characters to show before and after the match\n",
    "context_delta = 20\n",
    "with open('results.csv', 'w') as new_file:\n",
    "    results = csv.writer(new_file)\n",
    "    rows = [['File', 'Line', 'Index', 'Context']]\n",
    "    for key in science_match:\n",
    "        with open(os.path.join('wiki', key)) as file_name:\n",
    "            file_name = list(file_name)\n",
    "            for line_index, index in science_match[key]:\n",
    "                start_context = max(index - context_delta, 0)\n",
    "                end_context = index + len(str_given) + context_delta\n",
    "                context = file_name[line_index][start_context:end_context]\n",
    "                rows.append([key, line_index, index, context])\n",
    "                \n",
    "    results.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Line</th>\n",
       "      <th>Index</th>\n",
       "      <th>Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Valentin_Yanin.html</td>\n",
       "      <td>6</td>\n",
       "      <td>840</td>\n",
       "      <td>the USSR Academy of Sciences\",\"Full Members of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Valentin_Yanin.html</td>\n",
       "      <td>6</td>\n",
       "      <td>890</td>\n",
       "      <td>Russian Academy of Sciences\",\"Demidov Prize la</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Valentin_Yanin.html</td>\n",
       "      <td>66</td>\n",
       "      <td>90</td>\n",
       "      <td>i/Soviet_Academy_of_Sciences\" class=\"mw-redirec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Valentin_Yanin.html</td>\n",
       "      <td>66</td>\n",
       "      <td>145</td>\n",
       "      <td>=\"Soviet Academy of Sciences\"&gt;Soviet Academy of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Valentin_Yanin.html</td>\n",
       "      <td>66</td>\n",
       "      <td>173</td>\n",
       "      <td>\"&gt;Soviet Academy of Sciences&lt;/a&gt;; he became a f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Valentin_Yanin.html</td>\n",
       "      <td>144</td>\n",
       "      <td>1440</td>\n",
       "      <td>the_USSR_Academy_of_Sciences\" title=\"Category:F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Valentin_Yanin.html</td>\n",
       "      <td>144</td>\n",
       "      <td>1502</td>\n",
       "      <td>the USSR Academy of Sciences\"&gt;Full Members of t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Valentin_Yanin.html</td>\n",
       "      <td>144</td>\n",
       "      <td>1548</td>\n",
       "      <td>the USSR Academy of Sciences&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a hre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Valentin_Yanin.html</td>\n",
       "      <td>144</td>\n",
       "      <td>1632</td>\n",
       "      <td>_Russian_Academy_of_Sciences\" title=\"Category:F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Valentin_Yanin.html</td>\n",
       "      <td>144</td>\n",
       "      <td>1697</td>\n",
       "      <td>Russian Academy of Sciences\"&gt;Full Members of t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Valentin_Yanin.html</td>\n",
       "      <td>144</td>\n",
       "      <td>1746</td>\n",
       "      <td>Russian Academy of Sciences&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a hre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>William_Harvey_Lillard.html</td>\n",
       "      <td>80</td>\n",
       "      <td>166</td>\n",
       "      <td>D.D. (1910) &lt;i&gt;The Science, Art and Philosophy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Victor_S._Mamatey.html</td>\n",
       "      <td>48</td>\n",
       "      <td>682</td>\n",
       "      <td>College_of_Arts_and_Sciences\" title=\"Franklin C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Victor_S._Mamatey.html</td>\n",
       "      <td>48</td>\n",
       "      <td>728</td>\n",
       "      <td>College of Arts and Sciences\"&gt;Franklin College</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Victor_S._Mamatey.html</td>\n",
       "      <td>48</td>\n",
       "      <td>767</td>\n",
       "      <td>College of Arts and Sciences&lt;/a&gt;. In 1984 he re</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           File  Line  Index  \\\n",
       "0           Valentin_Yanin.html     6    840   \n",
       "1           Valentin_Yanin.html     6    890   \n",
       "2           Valentin_Yanin.html    66     90   \n",
       "3           Valentin_Yanin.html    66    145   \n",
       "4           Valentin_Yanin.html    66    173   \n",
       "5           Valentin_Yanin.html   144   1440   \n",
       "6           Valentin_Yanin.html   144   1502   \n",
       "7           Valentin_Yanin.html   144   1548   \n",
       "8           Valentin_Yanin.html   144   1632   \n",
       "9           Valentin_Yanin.html   144   1697   \n",
       "10          Valentin_Yanin.html   144   1746   \n",
       "11  William_Harvey_Lillard.html    80    166   \n",
       "12       Victor_S._Mamatey.html    48    682   \n",
       "13       Victor_S._Mamatey.html    48    728   \n",
       "14       Victor_S._Mamatey.html    48    767   \n",
       "\n",
       "                                            Context  \n",
       "0   the USSR Academy of Sciences\",\"Full Members of   \n",
       "1    Russian Academy of Sciences\",\"Demidov Prize la  \n",
       "2   i/Soviet_Academy_of_Sciences\" class=\"mw-redirec  \n",
       "3   =\"Soviet Academy of Sciences\">Soviet Academy of  \n",
       "4   \">Soviet Academy of Sciences</a>; he became a f  \n",
       "5   the_USSR_Academy_of_Sciences\" title=\"Category:F  \n",
       "6   the USSR Academy of Sciences\">Full Members of t  \n",
       "7   the USSR Academy of Sciences</a></li><li><a hre  \n",
       "8   _Russian_Academy_of_Sciences\" title=\"Category:F  \n",
       "9    Russian Academy of Sciences\">Full Members of t  \n",
       "10   Russian Academy of Sciences</a></li><li><a hre  \n",
       "11   D.D. (1910) <i>The Science, Art and Philosophy  \n",
       "12  College_of_Arts_and_Sciences\" title=\"Franklin C  \n",
       "13  College of Arts and Sciences\">Franklin College   \n",
       "14  College of Arts and Sciences</a>. In 1984 he re  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "visualization = pd.read_csv('results.csv')\n",
    "visualization.head(15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
